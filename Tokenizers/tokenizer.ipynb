{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to creat env\n",
    "# python -m venv myenv\n",
    "\n",
    "\n",
    "# # to activate env\n",
    "# myenv/Scripts/activate\n",
    "\n",
    "\n",
    "# # upgrade python version\n",
    "\n",
    "# ## another way\n",
    "# pip install virtualenv\n",
    "\n",
    "# virctualenv -p python3 virtual_env\n",
    "\n",
    "# virtual_env/Scripsts/activate \n",
    "\n",
    "# deactivate\n",
    "\n",
    "\n",
    "# #### Better way using anaconda\n",
    "# conda create -p venv1 python==3.20 -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process of breaking down text/ Documents into smallert units called tokens (can be in words, characters, subwords, or symbols, depending on type of method used)\n",
    "## When to use;\n",
    "    # 1) After text cleaning:\n",
    "    #     - once raw text is cleaned (removed punctiations, special chanracter, lowercasing & etx.) tokenizatio comes next\n",
    "    # 2) Befor Vextor Creation:\n",
    "    #     - Tokenization happen before converting text into the numbers, After tokenizeation techniques like word2vec, TFIDF, bag of words, embedding (word3vec, Glove,Transformers are used to convert tokens into vector that model can process)\n",
    "\n",
    "## Why?\n",
    "\n",
    "# Input preparation: Most NLP models requires discrete input units. \n",
    "# Granularity control: word level or character level   \n",
    "# Vocabulary management: IT helps creating & managing  vocubulary for language model\n",
    "# Feature Extraction: Tokes serves as feature for various NLP tasks\n",
    "# Dimensiaonality Reduction: By breaking text into tokens, we can represent words or subwords as numerical vectors, reducing the diamensionality of the input.\n",
    "\n",
    "# How? \n",
    "\n",
    "# Text Normalization: Thismay include converting text to lowercase, removing punctuations or handling special characters\n",
    "\n",
    "# Boundary detection: Identifting wheere one token ends and another begins, This could be based on whitespace, or more complex rules\n",
    "\n",
    "# Token Extraction: Sperating identified tokens from the original \n",
    "\n",
    "# Token processing: This may involve further processing like stemming, Lemmatization or subword tokenization\n",
    "\n",
    "# Let's take a simple sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Example:\n",
    "\n",
    "# Let's take a simple sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Word-level tokenization: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "# Character-level tokenization: [\"T\", \"h\", \"e\", \" \", \"q\", \"u\", \"i\", \"c\", \"k\", \" \", \"b\", \"r\", \"o\", \"w\", \"n\", \" \", \"f\", \"o\", \"x\", \" \", \"j\", \"u\", \"m\", \"p\", \"s\", \" \", \"o\", \"v\", \"e\", \"r\", \" \", \"t\", \"h\", \"e\", \" \", \"l\", \"a\", \"z\", \"y\", \" \", \"d\", \"o\", \"g\"]\n",
    "\n",
    "# Subword tokenization (using BPE as an example): [\"The\", \"quick\", \"brown\", \"fox\", \"jump\", \"##s\", \"over\", \"the\", \"lazy\", \"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character level:  \n",
      " ['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.'] \n",
      "\n",
      "word level:  \n",
      " ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "text= \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "print(\"Character level: \", \"\\n\", list(text), \"\\n\")\n",
    "\n",
    "print(\"word level: \", \"\\n\",text.split() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGNIFICANCE\n",
    " Tokenization plays a crucial role in NLP for several reasons:\n",
    "\n",
    "   - **Language Understanding**: By breaking text into meaningful units, tokenization helps machines better understand and process human language.\n",
    "\n",
    "   - **Vocabulary Size Management**: It allows for control over the vocabulary size, which is crucial for model efficiency and handling out-of-vocabulary words.\n",
    "\n",
    "   - **Cross-lingual Applications**: Some tokenization methods (like BPE) can work across multiple languages, facilitating multilingual NLP models.\n",
    "\n",
    "   - **Handling of Rare Words**: Subword tokenization methods can effectively handle rare words by breaking them into more common subword units.\n",
    "\n",
    "   - **Improved Model Performance**: Proper tokenization can lead to better model performance by providing more meaningful input representations.\n",
    "\n",
    "   - **Consistency**: It ensures consistency in how text is processed, which is essential for reproducible results in NLP tasks.\n",
    "\n",
    "   - **Efficiency**: By converting text into numerical representations, tokenization enables efficient processing of large amounts of text data.\n",
    "\n",
    "   - **Feature Engineering**: Tokens serve as the basis for many feature engineering techniques in NLP, such as bag-of-words, TF-IDF, and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer type\n",
    "\n",
    "# Character tokenizer\n",
    "\n",
    "text= \"Hello World!\"\n",
    "print(list(text))\n",
    "# ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', '!']\n",
    "\n",
    "\n",
    "# advantage\n",
    "# 1) simple & easy to impliment\n",
    "# 2) No out of vocabulary issue\n",
    "# 3) useful for tasks requuired character level analysis \n",
    "\n",
    "# # Disadvantages:\n",
    "# 1) Loses word-level symantics\n",
    "# 2) Result in very long sequence which can be compuatationaly expensive\n",
    "# 3) May not capture higher level language strucutre effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level tokenizer\n",
    "#  word-level tokenizer splits text into individual words. It typically uses spaces and punctuation as delimiters to identify word boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Input: \"The quick brown fox jumps over the lazy dog.\" Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "- Advantages:\n",
    "    - Preserves word-level semantics\n",
    "    - Intuitive and easy to interpret\n",
    "    - Works well for many NLP tasks\n",
    "- Disadvantages:\n",
    "    - Large vocabulary size, especially for morphologically rich languages\n",
    "    - Cannot handle out-of-vocabulary words\n",
    "    - May struggle with compound words or unconventional spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shubham/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "# \n",
    "\n",
    "# print(tokens)\n",
    "# ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitespace Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Explanation:\n",
    "\n",
    "A whitespace tokenizer simply splits text on whitespace characters (spaces, tabs, newlines). It's one of the simplest forms of tokenization.\n",
    "\n",
    "**Example:*\n",
    "Input: \"The quick brown fox\\njumps over the lazy dog.\" Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\"]\n",
    "\n",
    " **Advantages:**\n",
    "- Very simple and fast\n",
    "- Works well for languages with clear word boundaries\n",
    "\n",
    " **Disadvantages:**\n",
    "- Doesn't handle punctuation well\n",
    "- May not work properly for languages without clear word boundaries (e.g., Chinese)\n",
    "- Can't handle contractions or hyphenated words effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'world!', 'This', 'is', 'an', 'example', 'text.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "text = \"Hello, world! This is an example text.\"\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!', 'This', 'is', 'an', 'example', 'text.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubWord Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubWord tokenization methods aim to break words into smaller meaningful units, helping to balance vocabulary size and semantic representation.\n",
    "\n",
    "## **Subword Tokenization Methods**\n",
    "\n",
    "Subword tokenization is a crucial technique in Natural Language Processing (NLP) that breaks words into smaller units. This approach helps handle out-of-vocabulary words, reduces vocabulary size, and captures morphological information. Here, we'll discuss four important subword tokenization methods: Byte-Pair Encoding (BPE), WordPiece, Unigram, and SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Byte-Pair Encoding (BPE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working Explanation**\n",
    "BPE starts with a vocabulary of individual characters and iteratively merges the most frequent adjacent pairs of characters or subwords. The process continues until a desired vocabulary size is reached.\n",
    "\n",
    "    1) Initialize the vocabulary with individual characters.\n",
    "    2) Count the frequency of character pairs in the corpus.\n",
    "    3) Merge the most frequent pair and add it to the vocabulary.\n",
    "    4) Repeat steps 2-3 until the desired vocabulary size is reached.\n",
    "    \n",
    "Example:\n",
    "\n",
    "    Initial words: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "    Base vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]\n",
    "    After merges: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "\n",
    "**Advantages**\n",
    "    - Effective balance between vocabulary size and token expressiveness\n",
    "\n",
    "    - Handles rare words and OOV words well\n",
    "\n",
    "    - Can capture subword semantics\n",
    "\n",
    "    - Works well for multilingual models\n",
    "\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "    - Can produce unintuitive splits for some words\n",
    "\n",
    "    - Requires training on a corpus\n",
    "\n",
    "    - May not always capture morphological structures effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shubham/anaconda3/envs/pytor/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
      "Downloading tokenizers-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.2 tokenizers-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CORPUS TEXT\n",
    "# In the beginning, machines were designed to follow human instructions.\n",
    "# However, with the advent of artificial intelligence, machines have started learning on their own.\n",
    "# This corpus is a collection of sentences to train the BasicTokenizer.\n",
    "# Feel free to expand this file with more data to make the tokenizer better.\n",
    "# Natural language processing is the key to many AI-powered applications.\n",
    "# Tokenizers play a crucial role in transforming text into machine-readable formats.\n",
    "# Machine learning models often require text to be split into smaller chunks called tokens.\n",
    "# This process is essential in many NLP tasks such as sentiment analysis, translation, and summarization.\n",
    "# By training a tokenizer, we can effectively break down sentences into meaningful units .\n",
    "# With more training data, the tokenizer becomes more efficient at handling complex sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\" BPE Tokenizers use huggingFace tokenizers Library\"\"\"\n",
    "    def __init__(self, vocab_size=1000) -> None:\n",
    "        self.tokenizer= Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                                  vocab_size=vocab_size,\n",
    "                                  show_progress=True\n",
    "                                  )\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    def train(self, files):\n",
    "        \"\"\"Train the BPE Tokenizer on Given corpus\"\"\"\n",
    "        # https://huggingface.co/docs/tokenizers/en/api/tokenizer#tokenizers.Tokenizer.train\n",
    "        self.tokenizer.train(files=files, trainer= self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize the inputtext using trained BPE Tokenizer \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "bpe_tokenizer = BPETokenizer()\n",
    "corpus= ['./res/corpus.txt']\n",
    "bpe_tokenizer.train(files=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'the',\n",
       " 'beginning',\n",
       " ',',\n",
       " 'machines',\n",
       " 'were',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'human',\n",
       " 'instructions',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(\"In the beginning, machines were designed to follow human instructions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'el', 'l', 'o', ' s', 'h', 'u', 'b', 'h', 'a', 'm']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(\"Hello shubham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe_tokenizer.tokenize(\"Hello world\")\n",
    "# ['Hello', 'w', 'o', 'r', 'l', 'd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'el', 'l', 'o', 'w', 'o', 'r', 'l', 'd']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With', 'more', 'training', 'data', ',', 'the', 'tokenizer', 'becomes', 'more', 'efficient', 'at', 'handling', 'complex', 'sentences']\n"
     ]
    }
   ],
   "source": [
    "text = 'With more training data, the tokenizer becomes more efficient at handling complex sentences'\n",
    "print(bpe_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'i', '[UNK]', 'u', 'y', 's', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "text = 'With more training data, the tokenizer becomes more efficient at handling complex sentences'\n",
    "text2 = \"Hi Guys!\"\n",
    "## as the tokenizer not trained on \"G\" capital G, so it's resulting as unknown token\n",
    "print(bpe_tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWith more training data, the tokenizer becomes more efficient at handling complex sentences\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi Guys!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbpe_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m, in \u001b[0;36mBPETokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize the inputtext using trained BPE Tokenizer \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "text = 'With more training data, the tokenizer becomes more efficient at handling complex sentences'\n",
    "text2 = \"Hi Guys!\"\n",
    "print(bpe_tokenizer.tokenize([text, text2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This ', 'is ', 'a', ' s', 'a', 'm', 'p', 'le ', 's', 'entence', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = bpe_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "WordPiece is similar to BPE but uses a different criterion for merging tokens. Instead of choosing the most frequent pair, it selects the pair that maximizes the likelihood of the training data when added to the vocabulary.\n",
    "\n",
    "1. Initialize the vocabulary with individual characters.\n",
    "2. For each possible merge, calculate the increase in likelihood of the training data.\n",
    "3. Choose the merge that results in the highest increase in likelihood.\n",
    "4. Repeat steps 2-3 until the desired vocabulary size is reached.\n",
    "\n",
    "##### **Advantages**\n",
    "1. Often produces more meaningful subword units\n",
    "2. Balances frequency and usefulness of tokens\n",
    "3. Effective for languages with rich morphology\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. Can be computationally more expensive than BPE\n",
    "2. Still requires a pre-tokenization step for most implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['This', 'is', 'a', 's', '##a', '##mp', '##le', 'sent', '##ence', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class WordPieceTokenizerHF:\n",
    "    \"\"\"\n",
    "    WordPiece tokenizer using HuggingFace Tokenizers library.\n",
    "    You can add normalization for text preprocessing..\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files):\n",
    "        \"\"\"\n",
    "        Train the WordPiece tokenizer on a given corpus.\n",
    "        \"\"\"\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text using the trained WordPiece tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "# Example usage\n",
    "wordpiece_tokenizer = WordPieceTokenizerHF(vocab_size=2000)\n",
    "corpus = [\"res/corpus.txt\"]  # Provide your corpus file here\n",
    "wordpiece_tokenizer.train(corpus)\n",
    "tokens = wordpiece_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'i', ' ', '[UNK]', 'u', 'y', 's', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "## BPE Result\n",
    "text2 = \"Hi Guys!\"\n",
    "print(bpe_tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', '##i', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# wrodpiece result\n",
    "text2 = \"Hi Guys!\"\n",
    "print(wordpiece_tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unigram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "Unigram starts with a large vocabulary and iteratively removes tokens to reach the desired vocabulary size.\n",
    "\n",
    "1. Initialize with a large vocabulary (e.g., all pre-tokenized words and common substrings).\n",
    "2. Define a loss function over the training data given the current vocabulary.\n",
    "3. For each symbol, calculate the loss increase if it were removed.\n",
    "4. Remove a percentage of symbols with the lowest loss increase.\n",
    "5. Repeat steps 2-4 until the desired vocabulary size is reached.\n",
    "\n",
    "##### **Advantages**\n",
    "1. Allows for multiple tokenization possibilities, which can improve robustness\n",
    "2. Can find an optimal vocabulary for a given size\n",
    "3. Works well with SentencePiece for language-agnostic tokenization\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. More complex implementation compared to BPE or WordPiece\n",
    "2. May require more computational resources during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['T', 'hi', 's', 'i', 's', 'a', 's', 'a', 'm', 'p', 'le', 'sent', 'ence', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class UnigramTokenizerHF:\n",
    "    \"\"\"\n",
    "    Unigram tokenizer using HuggingFace Tokenizers library.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(Unigram())\n",
    "        self.trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files):\n",
    "        \"\"\"\n",
    "        Train the Unigram tokenizer on a given corpus.\n",
    "        \"\"\"\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text using the trained Unigram tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "# Example usage\n",
    "unigram_tokenizer = UnigramTokenizerHF(vocab_size=2000)\n",
    "corpus = [\"res/corpus.txt\"]  # Provide your corpus file here\n",
    "unigram_tokenizer.train(corpus)\n",
    "tokens = unigram_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "SentencePiece is not a tokenization algorithm itself, but rather a framework that can use BPE or Unigram algorithms. Its key feature is treating the input as a raw stream, including spaces as part of the token set.\n",
    "\n",
    "1. Treat the input text as a raw stream of characters, including spaces.\n",
    "2. Apply either BPE or Unigram algorithm to this stream.\n",
    "3. Learn a vocabulary that includes space-separated tokens.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Input: \"Hello world\"\n",
    "Tokenized: [\"▁Hello\", \"▁world\"]\n",
    "```\n",
    "(Note: \"▁\" represents the space character)\n",
    "\n",
    "##### **Advantages**\n",
    "1. Language-agnostic: works well for languages without clear word boundaries\n",
    "2. Reversible tokenization: easy to reconstruct the original text\n",
    "3. Consistent tokenization across languages in multilingual models\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. May produce tokens that don't align with linguistic units in some languages\n",
    "2. Can be less intuitive for debugging or analysis compared to word-based tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **BPE**: Good general-purpose algorithm, widely used (e.g., GPT models)\n",
    "2. **WordPiece**: Effective for morphologically rich languages, used in BERT and related models\n",
    "3. **Unigram**: Offers probabilistic tokenization, good for handling ambiguity\n",
    "4. **SentencePiece**: Excellent for multilingual models and languages without clear word boundaries\n",
    "\n",
    "When choosing a subword tokenization method, consider factors such as the language(s) you're working with, the size of your corpus, computational resources, and the specific requirements of your NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
