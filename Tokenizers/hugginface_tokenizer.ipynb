{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When calling Tokenizer.encode or Tokenizer.encode_batch, the input text(s) go through the following pipeline:\n",
    "\n",
    "    ##### normalization\n",
    "    ##### pre-tokenization\n",
    "    ##### model\n",
    "    ##### post-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/tokenizer-wiki.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When building a Tokenizer, you can customize its normalizer by just changing the corresponding attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretokenization\n",
    "\n",
    "\n",
    "Pre-tokenization is the act of splitting a text into smaller objects that give an upper bound to what your tokens will be at the end of training. A good way to think of this is that the pre-tokenizer will split your text into ‚Äúwords‚Äù and then, your final tokens will be parts of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('!', (5, 6)),\n",
       " ('How', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (15, 18)),\n",
       " ('?', (18, 19)),\n",
       " ('I', (20, 21)),\n",
       " (\"'\", (21, 22)),\n",
       " ('m', (22, 23)),\n",
       " ('fine', (24, 28)),\n",
       " (',', (28, 29)),\n",
       " ('thank', (30, 35)),\n",
       " ('you', (36, 39)),\n",
       " ('.', (39, 40))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer\n",
    "\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\n",
    "# [(\"Hello\", (0, 5)), (\"!\", (5, 6)), (\"How\", (7, 10)), (\"are\", (11, 14)), (\"you\", (15, 18)),\n",
    "#  (\"?\", (18, 19)), (\"I\", (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), (\"fine\", (24, 28)),\n",
    "#  (\",\", (28, 29)), (\"thank\", (30, 35)), (\"you\", (36, 39)), (\".\", (39, 40))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('!', (5, 6)),\n",
       " ('How', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (15, 18)),\n",
       " ('?', (18, 19)),\n",
       " ('I', (20, 21)),\n",
       " (\"'\", (21, 22)),\n",
       " ('m', (22, 23)),\n",
       " ('fine', (24, 28)),\n",
       " (',', (28, 29)),\n",
       " ('thank', (30, 35)),\n",
       " ('you', (36, 39)),\n",
       " ('.', (39, 40))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = BertPreTokenizer()\n",
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")\n",
    "# [(\"Hello\", (0, 5)), (\"!\", (5, 6)), (\"How\", (7, 10)), (\"are\", (11, 14)), (\"you\", (15, 18)),\n",
    "#  (\"?\", (18, 19)), (\"I\", (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), (\"fine\", (24, 28)),\n",
    "#  (\",\", (28, 29)), (\"thank\", (30, 35)), (\"you\", (36, 39)), (\".\", (39, 40))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello!', (0, 6)),\n",
       " ('How', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you?', (15, 19)),\n",
       " (\"I'm\", (20, 23)),\n",
       " ('fine,', (24, 29)),\n",
       " ('thank', (30, 35)),\n",
       " ('you.', (36, 40))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "pre_tokenizer = WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!', 'How', 'are', 'you?', \"I'm\", 'fine,', 'thank', 'you.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello! How are you? I'm fine, thank you.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)), ('9', (5, 6)), ('1', (6, 7)), ('1', (7, 8)), ('!', (8, 9))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")\n",
    "# [(\"Call\", (0, 4)), (\"9\", (5, 6)), (\"1\", (6, 7)), (\"1\", (7, 8)), (\"!\", (8, 9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)), ('911', (5, 8)), ('!', (8, 9))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False)])\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")\n",
    "# [('Call', (0, 4)), ('911', (5, 8)), ('!', (8, 9))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the input texts are normalized and pre-tokenized, the Tokenizer applies the model on the pre-tokens. This is the part of the pipeline that needs training on your corpus (or that has been trained if you are using a pretrained tokenizer).\n",
    "\n",
    "# The role of the model is to split your ‚Äúwords‚Äù into tokens, using the rules it has learned. It‚Äôs also responsible for mapping those tokens to their corresponding IDs in the vocabulary of the model.\n",
    "\n",
    "# This model is passed along when intializing the Tokenizer so you already know how to customize this part. Currently, the ü§ó Tokenizers library supports:\n",
    "\n",
    "# models.BPE\n",
    "# models.Unigram\n",
    "# models.WordLevel\n",
    "# models.WordPiece\n",
    "# For more details about each model and its behavior, you can check here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing is the last step of the tokenization pipeline, to perform any additional transformation to the Encoding before it‚Äôs returned, like adding potential special tokens.\n",
    "\n",
    "# As we saw in the quick tour, we can customize the post processor of a Tokenizer by setting the corresponding attribute. For instance, here is how we can post-process to make the inputs suitable for the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= Tokenizer(tokenizers.models.WordPiece())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All togather: Bert tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "# Normalization\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretokenization\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Read the Parquet file\n",
    "# df = pd.read_parquet('res/validation-00000-of-00001.parquet', engine='pyarrow')  # or engine='fastparquet'\n",
    "\n",
    "# # Step 2: Convert the DataFrame to a string\n",
    "# df_string = df.to_string(index=False)\n",
    "\n",
    "# # Step 3: Write the string to a text file\n",
    "# with open('res/wiki.txt', 'w') as f:\n",
    "#     f.write(df_string)\n",
    "\n",
    "# print(\"Data written to output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "trainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "# files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "files = [\"res/wiki.txt\"]\n",
    "# files = [\"res/corpus.txt\"]\n",
    "bert_tokenizer.train(files, trainer)\n",
    "# bert_tokenizer.save(\"./bert-corpus.json\")\n",
    "bert_tokenizer.save(\"./bert-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.ids)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"welcome to the tokenizers library.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
